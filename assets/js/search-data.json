{
  
    
        "post0": {
            "title": "Dive into Deep Learning",
            "content": "For this blog (project), we want to train a deep learning model to classify images of different insects (beetles, cockroaches, dragonflies) . The neural network and its classification we used will be explained by Shapley Additive Explanations. . We first import all the libraries we want to use for this project. . Out neural network is used under tensorflow/keras. . Other libraries we used include but not limited to: matplotlib, numpy, skimage etc. . %matplotlib inline import tensorflow as tf import matplotlib as plt import numpy as np import cv2 import os import PIL from PIL import Image import glob import skimage from skimage import data from matplotlib import pyplot as plt %pylab inline from tensorflow.keras import Model from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications import MobileNetV2 import pickle . Populating the interactive namespace from numpy and matplotlib . from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.optimizers import RMSprop . We first load one of the image that we want to train and check out how it looks like. . img=image.load_img(&quot;insects/train/beetles/5556579.jpg&quot;) plt.imshow(img) plt.show(img) . Ok it appears...very interesting (disgusting). Let&#39;s try not to open anyone of them again...... . . Let&#39;s see the shape of one example just to get an idea of its measures. . cv2.imread(&quot;insects/train/beetles/5556636.jpg&quot;).shape . (326, 384, 3) . Cool, now we know how they look like and their shapes, let&#39;s prepare for our models. . from tensorflow.keras.preprocessing.image import ImageDataGenerator . Let&#39;s create a training data generator using ImageDataGenerator from tensorflow.keras. . We want to do some pre-processing and adjust for our images, then we want to separate both a training set and a validation set (80%/20% split). . train_datagen = ImageDataGenerator(rotation_range=15, rescale=1./255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2) # set validation split . Now let&#39;s open the training dataset from our directory and adjust its sizes. . we also want to set the class_mode as categorical since we are not doing binary values. . train_dataset = train_datagen.flow_from_directory(&quot;insects/train&quot;, #shuffle=True, target_size= (200,200), batch_size = 3, class_mode = &#39;categorical&#39;, subset=&#39;training&#39;) . Found 816 images belonging to 3 classes. . Looks like we have 816 images with 3 classes. That&#39;s exactly what we want. Now let&#39;s keep going... . Same steps with the validation dataset. . validation_dataset = train_datagen.flow_from_directory(&quot;insects/train&quot;, #shuffle=True, target_size= (200,200), batch_size = 3, class_mode = &#39;categorical&#39;, subset=&#39;validation&#39; ) . Found 203 images belonging to 3 classes. . 203 images with also 3 classes, great! . Now, let&#39;s declare out testing data generator! Same steps as above... . test_datagen = ImageDataGenerator(rotation_range=15, rescale=1./255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1) test_generator = test_datagen.flow_from_directory(&quot;insects/train&quot;,#shuffle=True, target_size=(200,200), class_mode=&#39;categorical&#39;, batch_size=3) . Found 1019 images belonging to 3 classes. . Good, we now have 1019 images with also 3 classes in out testing set. . Let&#39;s see what these 3 classes are: . train_dataset.class_indices . {&#39;beetles&#39;: 0, &#39;cockroach&#39;: 1, &#39;dragonflies&#39;: 2} . validation_dataset.class_indices . {&#39;beetles&#39;: 0, &#39;cockroach&#39;: 1, &#39;dragonflies&#39;: 2} . . . Cool, looks like our 3 classes are: beetles as 0, cockroach as 1, and dragonflies as 2! . Let&#39;s do something for our model! For this model, we used a neural network that has a convolution 2D neural net structure. . How do we build it? . We declare different layers sequentially so that every layer has its shape and dimensions. We then dense and flatten the layers. . model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation=&#39;relu&#39;,input_shape=(200,200,3)), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Conv2D(64,(3,3),activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Conv2D(128,(3,3),activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512,activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(3,activation=&#39;softmax&#39;) ]) . We also want to minimized our loss function which is categorical_crossentropy in this case. . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer = RMSprop(lr=0.001), metrics=[&#39;accuracy&#39;] ) . Looks like everything is set now, let&#39;s start fitting our model! . history=model.fit(train_dataset, steps_per_epoch = 5, batch_size = 1, epochs = 30, validation_data = validation_dataset ) . Epoch 1/30 5/5 [==============================] - 5s 1s/step - loss: 1.1644 - accuracy: 0.6000 - val_loss: 2.6884 - val_accuracy: 0.3941 Epoch 2/30 5/5 [==============================] - 5s 1s/step - loss: 0.8894 - accuracy: 0.6000 - val_loss: 2.4192 - val_accuracy: 0.4138 Epoch 3/30 5/5 [==============================] - 5s 1s/step - loss: 1.4179 - accuracy: 0.4000 - val_loss: 2.8435 - val_accuracy: 0.3251 Epoch 4/30 5/5 [==============================] - 5s 1s/step - loss: 0.9131 - accuracy: 0.6667 - val_loss: 1.4584 - val_accuracy: 0.5419 Epoch 5/30 5/5 [==============================] - 5s 1s/step - loss: 0.8746 - accuracy: 0.7333 - val_loss: 1.9552 - val_accuracy: 0.3793 Epoch 6/30 5/5 [==============================] - 5s 1s/step - loss: 0.5609 - accuracy: 0.8000 - val_loss: 2.2928 - val_accuracy: 0.3202 Epoch 7/30 5/5 [==============================] - 5s 1s/step - loss: 1.0883 - accuracy: 0.7333 - val_loss: 1.9884 - val_accuracy: 0.3596 Epoch 8/30 5/5 [==============================] - 5s 1s/step - loss: 0.8982 - accuracy: 0.6000 - val_loss: 3.0755 - val_accuracy: 0.3202 Epoch 9/30 5/5 [==============================] - 5s 1s/step - loss: 1.2994 - accuracy: 0.6667 - val_loss: 2.8295 - val_accuracy: 0.3153 Epoch 10/30 5/5 [==============================] - 5s 1s/step - loss: 1.0217 - accuracy: 0.6000 - val_loss: 3.2342 - val_accuracy: 0.3842 Epoch 11/30 5/5 [==============================] - 5s 1s/step - loss: 1.7757 - accuracy: 0.5333 - val_loss: 3.6724 - val_accuracy: 0.3103 Epoch 12/30 5/5 [==============================] - 6s 1s/step - loss: 0.6816 - accuracy: 0.6667 - val_loss: 2.0861 - val_accuracy: 0.3350 Epoch 13/30 5/5 [==============================] - 6s 1s/step - loss: 0.9954 - accuracy: 0.6667 - val_loss: 3.5514 - val_accuracy: 0.3103 Epoch 14/30 5/5 [==============================] - 6s 1s/step - loss: 0.9269 - accuracy: 0.5333 - val_loss: 1.8490 - val_accuracy: 0.4089 Epoch 15/30 5/5 [==============================] - 6s 1s/step - loss: 1.3209 - accuracy: 0.7333 - val_loss: 3.1780 - val_accuracy: 0.3103 Epoch 16/30 5/5 [==============================] - 6s 1s/step - loss: 1.2244 - accuracy: 0.6667 - val_loss: 3.5121 - val_accuracy: 0.3941 Epoch 17/30 5/5 [==============================] - 6s 1s/step - loss: 1.6605 - accuracy: 0.4667 - val_loss: 4.1140 - val_accuracy: 0.3350 Epoch 18/30 5/5 [==============================] - 6s 1s/step - loss: 1.1923 - accuracy: 0.5333 - val_loss: 3.6464 - val_accuracy: 0.3103 Epoch 19/30 5/5 [==============================] - 6s 1s/step - loss: 1.2381 - accuracy: 0.4000 - val_loss: 4.6172 - val_accuracy: 0.3103 Epoch 20/30 5/5 [==============================] - 5s 1s/step - loss: 1.1144 - accuracy: 0.6667 - val_loss: 3.4138 - val_accuracy: 0.3103 Epoch 21/30 5/5 [==============================] - 5s 1s/step - loss: 1.1225 - accuracy: 0.6667 - val_loss: 3.4383 - val_accuracy: 0.3547 Epoch 22/30 5/5 [==============================] - 5s 1s/step - loss: 1.6811 - accuracy: 0.4667 - val_loss: 2.1082 - val_accuracy: 0.3202 Epoch 23/30 5/5 [==============================] - 5s 1s/step - loss: 1.5214 - accuracy: 0.6667 - val_loss: 2.3937 - val_accuracy: 0.3744 Epoch 24/30 5/5 [==============================] - 5s 1s/step - loss: 1.1620 - accuracy: 0.6000 - val_loss: 2.1490 - val_accuracy: 0.3695 Epoch 25/30 5/5 [==============================] - 5s 1s/step - loss: 1.6265 - accuracy: 0.4667 - val_loss: 2.7069 - val_accuracy: 0.3153 Epoch 26/30 5/5 [==============================] - 5s 1s/step - loss: 0.4719 - accuracy: 0.8667 - val_loss: 2.2129 - val_accuracy: 0.3892 Epoch 27/30 5/5 [==============================] - 5s 1s/step - loss: 1.2966 - accuracy: 0.6000 - val_loss: 2.1819 - val_accuracy: 0.3645 Epoch 28/30 5/5 [==============================] - 5s 1s/step - loss: 1.5784 - accuracy: 0.5333 - val_loss: 1.5188 - val_accuracy: 0.4384 Epoch 29/30 5/5 [==============================] - 5s 1s/step - loss: 1.0030 - accuracy: 0.6667 - val_loss: 1.5709 - val_accuracy: 0.3941 Epoch 30/30 5/5 [==============================] - 5s 1s/step - loss: 1.6075 - accuracy: 0.3333 - val_loss: 1.2297 - val_accuracy: 0.5714 . Done! Luckily, even if my laptop was heating up and screaming, it didn&#39;t die after this. . Let&#39;s see how our training loss and validation loss look like in the model: . loss_train = history.history[&#39;loss&#39;] loss_val = history.history[&#39;val_loss&#39;] epochs = range(1,31) plt.plot(epochs, loss_train, &#39;g&#39;, label=&#39;Training loss&#39;) plt.plot(epochs, loss_val, &#39;b&#39;, label=&#39;validation loss&#39;) plt.title(&#39;Training and Validation loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() plt.show() . Honestly I think that looks okay, could be much better though but I don&#39;t want to keep torturing my laptop. . How do my training accuracy and validation accuracy look like? . loss_train = history.history[&#39;accuracy&#39;] loss_val = history.history[&#39;val_accuracy&#39;] epochs = range(1,31) plt.plot(epochs, loss_train, &#39;g&#39;, label=&#39;Training accuracy&#39;) plt.plot(epochs, loss_val, &#39;b&#39;, label=&#39;validation accuracy&#39;) plt.title(&#39;Training and Validation accuracy&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.legend() plt.show() . Ok...Let&#39;s see some summaries of our model: . model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_3 (Conv2D) (None, 198, 198, 32) 896 _________________________________________________________________ batch_normalization_4 (Batch (None, 198, 198, 32) 128 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 99, 99, 32) 0 _________________________________________________________________ dropout_4 (Dropout) (None, 99, 99, 32) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 97, 97, 64) 18496 _________________________________________________________________ batch_normalization_5 (Batch (None, 97, 97, 64) 256 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 48, 48, 64) 0 _________________________________________________________________ dropout_5 (Dropout) (None, 48, 48, 64) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 46, 46, 128) 73856 _________________________________________________________________ batch_normalization_6 (Batch (None, 46, 46, 128) 512 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 23, 23, 128) 0 _________________________________________________________________ dropout_6 (Dropout) (None, 23, 23, 128) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 67712) 0 _________________________________________________________________ dense_2 (Dense) (None, 512) 34669056 _________________________________________________________________ batch_normalization_7 (Batch (None, 512) 2048 _________________________________________________________________ dropout_7 (Dropout) (None, 512) 0 _________________________________________________________________ dense_3 (Dense) (None, 3) 1539 ================================================================= Total params: 34,766,787 Trainable params: 34,765,315 Non-trainable params: 1,472 _________________________________________________________________ . Cool. Now let&#39;s test our model and see how it reacts to the testing data. Let&#39;s try testing the beetles: . dir_path = &#39;insects/test/beetles&#39; for i in os.listdir(dir_path): img= image.load_img(dir_path+&#39;/&#39;+i, target_size=(200,200)) plt.imshow(img) plt.show() X=image.img_to_array(img) X= np.expand_dims(X, axis=0) images = np.vstack([X]) val=model.predict(images) if (val==0).any(): print(&quot;It is a beetle&quot;) else: print(&quot;Not a beetle&quot;) . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . It is a beetle . Good job! Our model detected all the beetles from our testing set. They look just as interesting (disgusting) as the one we&#39;ve seen not long ago. . Shapley Explanations: . We then explain how the neural network classified the images using shap(gradient explainer) . Converting datasets into numpy arrays: . x_train=np.concatenate([train_dataset.next()[0] for i in range(train_dataset.__len__())]) y_train=np.concatenate([train_dataset.next()[1] for i in range(train_dataset.__len__())]) x_test=np.concatenate([test_generator.next()[0] for i in range(test_generator.__len__())]) y_test=np.concatenate([test_generator.next()[1] for i in range(test_generator.__len__())]) y_test = np.where(ytest == 0, &quot;beetles&quot;, np.where(ytest == 1, &quot;cockroach&quot;, &quot;dragonflies&quot;)) . elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison . Declare shap explainer: . explainer = shap.GradientExplainer(model, xtrain) shap_vals = explainer.shap_values(xtest[:5]) . Let&#39;s see image plots from shap: . shap.image_plot([shap_vals[i] for i in range(3)], xtest[91:96]) . [&#39;beetles&#39;, &#39;cockroach&#39;,&#39;dragonflies&#39;, &#39;beetles&#39;,&#39;dragonflies&#39;] . Thank you for watching! .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/2021/11/14/HW5-Insects-Images-Deep-Learning.html",
            "relUrl": "/2021/11/14/HW5-Insects-Images-Deep-Learning.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "823 HW4 . Is there life after graduate school? . Download data of Science and Engineering PhDs awarded in the US. Do some analysis in pandas. Make a dashboard visualization of a few interesting aspects of the data. . click network URL: http://10.2.226.217:8501 for my dashboard visualization . import pandas as pd import numpy as np import plotly.express as px import altair as alt import matplotlib.pyplot as plt import seaborn as sns import streamlit as st . df001 = pd.read_excel(&quot;https://ncses.nsf.gov/pubs/nsf19301/assets/data/tables/sed17-sr-tab001.xlsx&quot;) df012= pd.read_excel(&quot;https://ncses.nsf.gov/pubs/nsf19301/assets/data/tables/sed17-sr-tab012.xlsx&quot;) df_unemployment=pd.read_excel(&quot;DataFinder-20211022004844.xlsx&quot;) . df_unemp=df_unemployment.iloc[17:].iloc[: , :-9].rename(columns={&quot;Labor Force Statistics from the Current Population Survey&quot;: &quot;Year&quot;, &quot;Unnamed: 1&quot;: &quot;Period&quot;,&quot;Unnamed: 2&quot;: &quot;Month&quot;,&quot;Unnamed: 3&quot;: &quot;Unemployment Rate&quot;}) df_unemp.head(20) df_unemp_group = df_unemp.loc[:1, &#39;Unemployment Rate&#39;].mean() . . df001 = df001.iloc[3:] #df001.head(15) df001.head(3) . Table 1 Unnamed: 1 Unnamed: 2 . 3 1958 | 8773 | - | . 4 1959 | 9213 | 5 | . 5 1960 | 9733 | 5.6 | . df001=df001.rename(columns={&quot;Table 1&quot;: &quot;Year&quot;, &quot;Unnamed: 1&quot;: &quot;Total Doctorate Recipients&quot;,&quot;Unnamed: 2&quot;: &quot;Percentage Change From Last Year&quot;}) df001[&#39;Change Percentage&#39;] = df001[&#39;Percentage Change From Last Year&#39;].astype(str) + &#39;%&#39; df001.head(10) . Year Total Doctorate Recipients Percentage Change From Last Year Change Percentage . 3 1958 | 8773 | - | -% | . 4 1959 | 9213 | 5 | 5% | . 5 1960 | 9733 | 5.6 | 5.6% | . 6 1961 | 10413 | 7 | 7% | . 7 1962 | 11500 | 10.4 | 10.4% | . 8 1963 | 12728 | 10.7 | 10.7% | . 9 1964 | 14325 | 12.5 | 12.5% | . 10 1965 | 16340 | 14.1 | 14.1% | . 11 1966 | 17949 | 9.8 | 9.8% | . 12 1967 | 20403 | 13.7 | 13.7% | . import plotly.express as px import plotly.graph_objects as go fig=px.line(df001, x=&quot;Year&quot;, y=&quot;Total Doctorate Recipients&quot;, title=&#39;Doctorate recipients from U.S. colleges and universities: 1958–2017&#39;) fig.show() fig2=px.scatter(df001, x=&#39;Year&#39;, y=&quot;Change Percentage&quot;,color=&#39;Year&#39;,title=&#39;Percentage change from last year&#39;) fig2.update_shapes(dict(xref=&#39;x&#39;, yref=&#39;y&#39;)) fig2.show() st.plotly_chart(fig, use_container_width=True) st.plotly_chart(fig2, use_container_width=True) . &lt;streamlit.delta_generator.DeltaGenerator at 0x7f4445e47a50&gt; . df012.head(10) . Table 12 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 . 0 Doctorate recipients, by major field of study:... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 (Number and percent) | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 Field of study | 1987 | NaN | 1992 | NaN | 1997 | NaN | 2002 | NaN | 2007 | NaN | 2012 | NaN | 2017 | NaN | . 3 NaN | Number | Percent | Number | Percent | Number | Percent | Number | Percent | Number | Percent | Number | Percent | Number | Percent | . 4 All fields | 32365 | 100 | 38886 | 100 | 42539 | 100 | 40031 | 100 | 48132 | 100 | 50944 | 100 | 54664 | 100 | . 5 Life sciences | 5783 | 17.9 | 7172 | 18.4 | 8421 | 19.8 | 8478 | 21.2 | 10702 | 22.2 | 11964 | 23.5 | 12592 | 23 | . 6 Agricultural sciences and natural resources | 1144 | 3.5 | 1261 | 3.2 | 1212 | 2.8 | 1129 | 2.8 | 1321 | 2.7 | 1255 | 2.5 | 1606 | 2.9 | . 7 Biological and biomedical sciences | 3839 | 11.9 | 4799 | 12.3 | 5788 | 13.6 | 5695 | 14.2 | 7238 | 15 | 8322 | 16.3 | 8477 | 15.5 | . 8 Health sciences | 800 | 2.5 | 1112 | 2.9 | 1421 | 3.3 | 1654 | 4.1 | 2143 | 4.5 | 2387 | 4.7 | 2509 | 4.6 | . 9 Physical sciences and earth sciences | 3811 | 11.8 | 4517 | 11.6 | 4550 | 10.7 | 3875 | 9.7 | 4956 | 10.3 | 5419 | 10.6 | 6081 | 11.1 | . df_17= pd.read_excel(&quot;https://ncses.nsf.gov/pubs/nsf19301/assets/data/tables/sed17-sr-tab017.xlsx&quot;, header=3) . df_17.head(50) . Field of study and citizenship status 1987 1992 1997 2002 2007 2012 2017 . 0 All fields | 32365 | 38886 | 42539 | 40031 | 48132 | 50944 | 54664 | . 1 U.S. citizen or permanent resident | 24585 | 28005 | 31097 | 27737 | 29501 | 32981 | 35791 | . 2 Temporary visa holder | 5660 | 9980 | 9194 | 9747 | 15123 | 14784 | 16323 | . 3 Unknown | 2120 | 901 | 2248 | 2547 | 3508 | 3179 | 2550 | . 4 Life sciencesa | 5783 | 7172 | 8421 | 8478 | 10702 | 11964 | 12592 | . 5 U.S. citizen or permanent resident | 4529 | 5106 | 5987 | 5850 | 7009 | 8184 | 8857 | . 6 Temporary visa holder | 939 | 1956 | 2058 | 2121 | 3039 | 3197 | 3329 | . 7 Unknown | 315 | 110 | 376 | 507 | 654 | 583 | 406 | . 8 Physical sciences and earth sciences | 3811 | 4517 | 4550 | 3875 | 4956 | 5419 | 6081 | . 9 U.S. citizen or permanent resident | 2657 | 2857 | 3034 | 2358 | 2567 | 3148 | 3717 | . 10 Temporary visa holder | 929 | 1572 | 1297 | 1330 | 2059 | 1959 | 2156 | . 11 Unknown | 225 | 88 | 219 | 187 | 330 | 312 | 208 | . 12 Mathematics and computer sciences | 1189 | 1927 | 2032 | 1729 | 3042 | 3496 | 3843 | . 13 U.S. citizen or permanent resident | 673 | 997 | 1150 | 868 | 1241 | 1627 | 1746 | . 14 Temporary visa holder | 445 | 876 | 771 | 789 | 1591 | 1617 | 1931 | . 15 Unknown | 71 | 54 | 111 | 72 | 210 | 252 | 166 | . 16 Psychology and social sciences | 6063 | 6562 | 7369 | 6925 | 7309 | 8498 | 9079 | . 17 U.S. citizen or permanent resident | 4846 | 5189 | 5796 | 5363 | 5272 | 6319 | 6874 | . 18 Temporary visa holder | 726 | 1158 | 1080 | 1110 | 1482 | 1601 | 1680 | . 19 Unknown | 491 | 215 | 493 | 452 | 555 | 578 | 525 | . 20 Engineering | 3712 | 5438 | 6114 | 5081 | 7749 | 8469 | 9843 | . 21 U.S. citizen or permanent resident | 1915 | 2521 | 3333 | 2169 | 2546 | 3579 | 4339 | . 22 Temporary visa holder | 1539 | 2749 | 2555 | 2650 | 4591 | 4355 | 5070 | . 23 Unknown | 258 | 168 | 226 | 262 | 612 | 535 | 434 | . 24 Education | 6453 | 6677 | 6577 | 6508 | 6448 | 4803 | 4823 | . 25 U.S. citizen or permanent resident | 5665 | 6018 | 5748 | 5417 | 5358 | 4040 | 4047 | . 26 Temporary visa holder | 430 | 559 | 411 | 479 | 601 | 460 | 536 | . 27 Unknown | 358 | 100 | 418 | 612 | 489 | 303 | 240 | . 28 Humanities and arts | 3478 | 4387 | 5285 | 5297 | 5085 | 5561 | 5290 | . 29 U.S. citizen or permanent resident | 2929 | 3745 | 4452 | 4320 | 3894 | 4434 | 4290 | . 30 Temporary visa holder | 282 | 542 | 561 | 705 | 797 | 761 | 693 | . 31 Unknown | 267 | 100 | 272 | 272 | 394 | 366 | 307 | . 32 Otherb | 1876 | 2206 | 2191 | 2138 | 2841 | 2734 | 3113 | . 33 U.S. citizen or permanent resident | 1371 | 1572 | 1597 | 1392 | 1614 | 1650 | 1921 | . 34 Temporary visa holder | 370 | 568 | 461 | 563 | 963 | 834 | 928 | . 35 Unknown | 135 | 66 | 133 | 183 | 264 | 250 | 264 | . indexNames = df_17[~(df_17[&#39;Field of study and citizenship status&#39;] != &#39;U.S. citizen or permanent resident&#39;)].index # Delete these row indexes from dataFrame df_17.drop(indexNames , inplace=True) indexNames2 = df_17[~(df_17[&#39;Field of study and citizenship status&#39;] != &#39;Temporary visa holder&#39;)].index # Delete these row indexes from dataFrame df_17.drop(indexNames2 , inplace=True) indexNames3 = df_17[~(df_17[&#39;Field of study and citizenship status&#39;] != &#39;Unknown&#39;)].index # Delete these row indexes from dataFrame df_17.drop(indexNames3 , inplace=True) df_17=df_17.rename(columns={&quot;Field of study and citizenship status&quot;: &quot;Field of Study&quot;}) df_17.head(10) . Field of Study 1987 1992 1997 2002 2007 2012 2017 . 0 All fields | 32365 | 38886 | 42539 | 40031 | 48132 | 50944 | 54664 | . 4 Life sciencesa | 5783 | 7172 | 8421 | 8478 | 10702 | 11964 | 12592 | . 8 Physical sciences and earth sciences | 3811 | 4517 | 4550 | 3875 | 4956 | 5419 | 6081 | . 12 Mathematics and computer sciences | 1189 | 1927 | 2032 | 1729 | 3042 | 3496 | 3843 | . 16 Psychology and social sciences | 6063 | 6562 | 7369 | 6925 | 7309 | 8498 | 9079 | . 20 Engineering | 3712 | 5438 | 6114 | 5081 | 7749 | 8469 | 9843 | . 24 Education | 6453 | 6677 | 6577 | 6508 | 6448 | 4803 | 4823 | . 28 Humanities and arts | 3478 | 4387 | 5285 | 5297 | 5085 | 5561 | 5290 | . 32 Otherb | 1876 | 2206 | 2191 | 2138 | 2841 | 2734 | 3113 | . #fig3.show() fig3 = px.bar(df_17, x=&#39;Field of Study&#39;, y=df_17.columns,title=&#39;Boxplot: Number of Doctorate Recipients by Study Fields Each Year in the United States&#39;) fig3.show() st.plotly_chart(fig3, use_container_width=True) . &lt;streamlit.delta_generator.DeltaGenerator at 0x7f4445e47a50&gt; . fig4 = px.line(df_17, x=&#39;Field of Study&#39;, y=df_17.columns,title=&#39;Lineplot: Trend of Doctorate Recipients by Study Fields in the United States&#39;) fig4.show() st.plotly_chart(fig4, use_container_width=True) . df_003= pd.read_excel(&quot;https://ncses.nsf.gov/pubs/nsf19301/assets/data/tables/sed17-sr-tab003.xlsx&quot;,skiprows=3) df_003.head(3) . Institution Rank Doctorate recipients . 0 U. Wisconsin-Madison | 1 | 844 | . 1 U. California, Berkeley | 2 | 799 | . 2 U. Texas, Austin | 3 | 795 | . fig6 = px.bar(df_003, x = &#39;Doctorate recipients&#39;, y = &#39;Rank&#39;, orientation=&#39;h&#39;, color=&#39;Doctorate recipients&#39;, text=&#39;Institution&#39;, template=&#39;plotly_dark&#39;, labels={&#39;Doctorate recipients&#39;: &#39;Number of Doctorate recipients&#39;}, width=800, height=400) fig6.show() st.plotly_chart(fig6, use_container_width=True) . &lt;streamlit.delta_generator.DeltaGenerator at 0x7f4445e47a50&gt; . st.write(&quot;&quot;&quot; Choropleth map to show the number of doctorate recipients ranked by state in 2017 &quot;&quot;&quot;) df_005= pd.read_excel(&quot;https://ncses.nsf.gov/pubs/nsf19301/assets/data/tables/sed17-sr-tab005.xlsx&quot;,skiprows=3) code = {&#39;Alabama&#39;: &#39;AL&#39;, &#39;Alaska&#39;: &#39;AK&#39;, &#39;Arizona&#39;: &#39;AZ&#39;, &#39;Arkansas&#39;: &#39;AR&#39;, &#39;California&#39;: &#39;CA&#39;, &#39;Colorado&#39;: &#39;CO&#39;, &#39;Connecticut&#39;: &#39;CT&#39;, &#39;Delaware&#39;: &#39;DE&#39;, &#39;District of Columbia&#39;: &#39;DC&#39;, &#39;Florida&#39;: &#39;FL&#39;, &#39;Georgia&#39;: &#39;GA&#39;, &#39;Hawaii&#39;: &#39;HI&#39;, &#39;Idaho&#39;: &#39;ID&#39;, &#39;Illinois&#39;: &#39;IL&#39;, &#39;Indiana&#39;: &#39;IN&#39;, &#39;Iowa&#39;: &#39;IA&#39;, &#39;Kansas&#39;: &#39;KS&#39;, &#39;Kentucky&#39;: &#39;KY&#39;, &#39;Louisiana&#39;: &#39;LA&#39;, &#39;Maine&#39;: &#39;ME&#39;, &#39;Maryland&#39;: &#39;MD&#39;, &#39;Massachusetts&#39;: &#39;MA&#39;, &#39;Michigan&#39;: &#39;MI&#39;, &#39;Minnesota&#39;: &#39;MN&#39;, &#39;Mississippi&#39;: &#39;MS&#39;, &#39;Missouri&#39;: &#39;MO&#39;, &#39;Montana&#39;: &#39;MT&#39;, &#39;Nebraska&#39;: &#39;NE&#39;, &#39;Nevada&#39;: &#39;NV&#39;, &#39;New Hampshire&#39;: &#39;NH&#39;, &#39;New Jersey&#39;: &#39;NJ&#39;, &#39;New Mexico&#39;: &#39;NM&#39;, &#39;New York&#39;: &#39;NY&#39;, &#39;North Carolina&#39;: &#39;NC&#39;, &#39;North Dakota&#39;: &#39;ND&#39;, &#39;Ohio&#39;: &#39;OH&#39;, &#39;Oklahoma&#39;: &#39;OK&#39;, &#39;Oregon&#39;: &#39;OR&#39;, &#39;Pennsylvania&#39;: &#39;PA&#39;, &#39;Rhode Island&#39;: &#39;RI&#39;, &#39;South Carolina&#39;: &#39;SC&#39;, &#39;South Dakota&#39;: &#39;SD&#39;, &#39;Tennessee&#39;: &#39;TN&#39;, &#39;Texas&#39;: &#39;TX&#39;, &#39;Utah&#39;: &#39;UT&#39;, &#39;Vermont&#39;: &#39;VT&#39;, &#39;Virginia&#39;: &#39;VA&#39;, &#39;Washington&#39;: &#39;WA&#39;, &#39;West Virginia&#39;: &#39;WV&#39;, &#39;Wisconsin&#39;: &#39;WI&#39;, &#39;Wyoming&#39;: &#39;WY&#39;} . df_005.head(5) df_005[&#39;Code&#39;] = df_005[&#39;State or location&#39;].map(code) . State or location Rank Doctorate recipients . 0 California | 1 | 6105 | . 1 Texas | 2 | 4068 | . 2 New York | 3 | 4064 | . 3 Massachusetts | 4 | 2879 | . 4 Pennsylvania | 5 | 2628 | . fig8 = go.Figure(data=go.Choropleth( locations=df_005[&#39;Code&#39;], z = df_005[&#39;Doctorate recipients&#39;].astype(float), locationmode = &#39;USA-states&#39;, # set of locations match entries in `locations colorscale = &#39;Viridis&#39;, colorbar_title = &quot;Total number of doctorate recipients &quot;, )) fig8.update_layout( title_text = &#39;2017 United States Doctorate Recipients Distribution Map&#39;, geo_scope=&#39;usa&#39;, # set map scope to USA ) fig8.show() st.plotly_chart(fig7, use_container_width=True) .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/2021/10/22/Yiyang-Zhang-823-HW4.html",
            "relUrl": "/2021/10/22/Yiyang-Zhang-823-HW4.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "823hw3",
            "content": "Creating effective visualizations using best practices: . Create 3 informative visualizations about malaria using Python in a Jupyter notebook, starting with the data sets at https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-13. . Where appropriate, make the visualizations interactive. . We have three original datasets: malaria_deaths.csv, malaria_deaths_age.csv, malaria_inc.csv. . The malaria_deaths dataset includes Entity, Code, Year and Deaths(per 100,000) . The malaria_deaths_age includes entity, code, year,age_group and death(total count) . The malaria_inc includes Entity, Code, Year and Incidence of malaria (per 1,000 population at risk) . After all three dataset have been read in and checked, I find myself particularly interested in seeing how the incidence changes within the most populated countires over the years. . So I looked up the list of populations per country and then chose 10 countries with most population that exist in the malaria_inc.csv dataset. . I store the data of these 10 countries in popn_top10 using pandas concatenate function. After that I renamed a column that has a very long and complicated name, with a simple descriptive name. . import pandas as pd import numpy as np import plotly.express as px . malaria_deaths = pd.read_csv(&quot;malaria_deaths.csv&quot;) malaria_deaths_age = pd.read_csv(&quot;malaria_deaths_age.csv&quot;, index_col=0) malaria_inc = pd.read_csv(&quot;malaria_inc.csv&quot;) malaria_deaths_age.rename(columns={&#39;entity&#39;:&#39;Entity&#39;,&#39;year&#39;:&#39;Year&#39;}, inplace=True) . popn_top10 =(pd.concat([malaria_inc[malaria_inc.Entity == &quot;China&quot;], malaria_inc[malaria_inc.Entity == &quot;India&quot;], malaria_inc[malaria_inc.Entity == &quot;Brazil&quot;], malaria_inc[malaria_inc.Entity == &quot;Indonesia&quot;], malaria_inc[malaria_inc.Entity == &quot;Pakistan&quot;], malaria_inc[malaria_inc.Entity == &quot;Nigeria&quot;], malaria_inc[malaria_inc.Entity == &quot;Bangladesh&quot;], malaria_inc[malaria_inc.Entity == &quot;Ethiopia&quot;], malaria_inc[malaria_inc.Entity == &quot;Mexico&quot;], malaria_inc[malaria_inc.Entity == &quot;Philippines&quot;],])) . Check how this dataframe looks . popn_top10.rename(columns={&#39;Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)&#39;:&#39;Incidence&#39;}, inplace=True) popn_top10.sample(10) . Entity Code Year Incidence . 332 Nigeria | NGA | 2000 | 497.800000 | . 79 China | CHN | 2015 | 0.000159 | . 340 Pakistan | PAK | 2000 | 44.800000 | . 223 Indonesia | IDN | 2015 | 26.100000 | . 22 Bangladesh | BGD | 2010 | 8.600000 | . 343 Pakistan | PAK | 2015 | 8.600000 | . 146 Ethiopia | ETH | 2010 | 106.200000 | . 45 Brazil | BRA | 2005 | 38.700000 | . 147 Ethiopia | ETH | 2015 | 58.600000 | . 21 Bangladesh | BGD | 2005 | 12.600000 | . After checking the dataframe, I decided to show an interactive barplot of how the incidence of malaria change in these most populated countries over the years. . The time interval is 5 years so we will see data from 2000,2005,2010 and 2015 . Feel free to zoom in to the plot and play with it. . fig1 = px.bar(popn_top10, x=&#39;Year&#39;, y=&#39;Incidence&#39;, color=&#39;Entity&#39;, color_discrete_sequence=[&#39;#2ca02c&#39;,&#39;#d62728&#39;,&#39;#9467bd&#39;,&#39;#8c564b&#39;,&#39;#e377c2&#39;,&#39;#1f77b4&#39;,&#39;#ff7f0e&#39;,&#39;#7f7f7f&#39;,&#39;#bcbd22&#39;,&#39;#17becf&#39;], title=&#39;Incidence interactive plot of Malaria in top 10 most populated countries&#39;) fig1.show() . After seeing the interactive barplot, I think adding a scatter plot will give people a more complete look on the trend of how incidence changes over the years. . Hence I did a scatter plot of the same dataset and added a linear trendline for each country. . Feel free to zoom in to the plot and play with it. . fig2 = px.scatter(popn_top10, x=&#39;Year&#39;, y=&#39;Incidence&#39;, color=&#39;Entity&#39;, color_discrete_sequence=[&#39;#2ca02c&#39;,&#39;#d62728&#39;,&#39;#9467bd&#39;,&#39;#8c564b&#39;,&#39;#e377c2&#39;,&#39;#1f77b4&#39;,&#39;#ff7f0e&#39;,&#39;#7f7f7f&#39;,&#39;#bcbd22&#39;,&#39;#17becf&#39;],trendline = &quot;ols&quot;, title=&#39;Incidence interactive plot of Malaria in top 10 most populated countries&#39;) fig2.show() . Now we have seen how the malaria incidence changes in the most populated countries over the years, but what about continets and other regions? . From the dataset, we can see the incidence of different continents and region that are also very informative. . I picked some of the most representative continents and special regions to redo a scatter plot with trend lines to see how malaria incidence changes in these places. . popn_cont_and_special =(pd.concat([malaria_inc[malaria_inc.Entity == &quot;Heavily indebted poor couuntries&quot;], malaria_inc[malaria_inc.Entity == &quot;East Asia &amp; Pacific&quot;], malaria_inc[malaria_inc.Entity == &quot;Latin America &amp; Caribbean&quot;], malaria_inc[malaria_inc.Entity == &quot;Sub-Saharan Africa&quot;], malaria_inc[malaria_inc.Entity == &quot;Fragile and conflict affected situations&quot;]]) ) popn_cont_and_special.rename(columns={&#39;Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)&#39;:&#39;Incidence&#39;}, inplace=True) popn_cont_and_special.sample(10) . Entity Code Year Incidence . 250 Latin America &amp; Caribbean | NaN | 2010 | 14.409069 | . 116 East Asia &amp; Pacific | NaN | 2000 | 22.736116 | . 118 East Asia &amp; Pacific | NaN | 2010 | 20.134525 | . 148 Fragile and conflict affected situations | NaN | 2000 | 319.032295 | . 249 Latin America &amp; Caribbean | NaN | 2005 | 25.144594 | . 150 Fragile and conflict affected situations | NaN | 2010 | 247.498298 | . 149 Fragile and conflict affected situations | NaN | 2005 | 304.801393 | . 251 Latin America &amp; Caribbean | NaN | 2015 | 10.026764 | . 423 Sub-Saharan Africa | NaN | 2015 | 234.292105 | . 420 Sub-Saharan Africa | NaN | 2000 | 422.510847 | . Feel free to zoom in to the plot and play with it. . fig3 = px.scatter(popn_cont_and_special, x=&#39;Year&#39;, y=&#39;Incidence&#39;, color=&#39;Entity&#39;, color_discrete_sequence=[&#39;#1f77b4&#39;,&#39;#ff7f0e&#39;,&#39;#7f7f7f&#39;,&#39;#bcbd22&#39;,&#39;#17becf&#39;],trendline = &quot;ols&quot;, title=&#39;Incidence interactive plot of Malaria in different continents and special regions&#39;) fig3.show() . We have looked at how malaria incidence changes in the most populated countries, different continents and special region. . Now let&#39;s see which country/continent or region has the most deaths over the years. . Since there are a few na values, we fill them forward and backward to ensure better fit. . df_merged = pd.merge(malaria_deaths, malaria_inc, how=&#39;left&#39;, left_on=[&#39;Entity&#39;, &#39;Year&#39;], right_on=[&#39;Entity&#39;, &#39;Year&#39;]) df_merged.rename(columns={&#39;Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)&#39;:&#39;Deaths&#39;, &#39;Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)&#39;:&#39;Incidence&#39;}, inplace=True) df_merged = df_merged.ffill().bfill() df_merged_all = pd.merge(df_merged, malaria_deaths_age, how=&#39;left&#39;,left_on=[&#39;Entity&#39;, &#39;Year&#39;], right_on=[&#39;Entity&#39;, &#39;Year&#39;]) df_merged_all.sort_values(by=[&#39;deaths&#39;]).tail(20) . Entity Code_x Year Deaths Code_y Incidence code age_group deaths . 25840 Sub-Saharan Africa | LKA | 2001 | 95.387495 | LKA | 422.510847 | NaN | Under 5 | 677487.264137 | . 30280 World | OWID_WRL | 1998 | 13.962049 | VNM | 0.300000 | OWID_WRL | Under 5 | 681065.266685 | . 25875 Sub-Saharan Africa | LKA | 2008 | 81.837780 | LKA | 354.424146 | NaN | Under 5 | 685669.900363 | . 30335 World | OWID_WRL | 2009 | 13.383327 | OWID_WRL | 141.256696 | OWID_WRL | Under 5 | 688065.379789 | . 25845 Sub-Saharan Africa | LKA | 2002 | 95.496177 | LKA | 422.510847 | NaN | Under 5 | 689351.595969 | . 25870 Sub-Saharan Africa | LKA | 2007 | 85.007071 | LKA | 354.424146 | NaN | Under 5 | 689897.819960 | . 25860 Sub-Saharan Africa | LKA | 2005 | 90.986663 | LKA | 354.424146 | NaN | Under 5 | 694321.203817 | . 25865 Sub-Saharan Africa | LKA | 2006 | 87.699550 | LKA | 354.424146 | NaN | Under 5 | 695767.937767 | . 30285 World | OWID_WRL | 1999 | 14.271505 | VNM | 0.300000 | OWID_WRL | Under 5 | 698846.092898 | . 25855 Sub-Saharan Africa | LKA | 2004 | 93.839664 | LKA | 422.510847 | NaN | Under 5 | 702419.646809 | . 30290 World | OWID_WRL | 2000 | 14.417081 | OWID_WRL | 158.124351 | OWID_WRL | Under 5 | 705882.728782 | . 25850 Sub-Saharan Africa | LKA | 2003 | 96.281593 | LKA | 422.510847 | NaN | Under 5 | 706059.061884 | . 30330 World | OWID_WRL | 2008 | 14.097223 | OWID_WRL | 141.256696 | OWID_WRL | Under 5 | 719208.349156 | . 30325 World | OWID_WRL | 2007 | 14.399868 | OWID_WRL | 141.256696 | OWID_WRL | Under 5 | 726367.804598 | . 30295 World | OWID_WRL | 2001 | 14.854815 | OWID_WRL | 158.124351 | OWID_WRL | Under 5 | 728254.356956 | . 30320 World | OWID_WRL | 2006 | 14.672998 | OWID_WRL | 141.256696 | OWID_WRL | Under 5 | 734869.501103 | . 30315 World | OWID_WRL | 2005 | 14.893298 | OWID_WRL | 141.256696 | OWID_WRL | Under 5 | 735966.857064 | . 30300 World | OWID_WRL | 2002 | 15.038252 | OWID_WRL | 158.124351 | OWID_WRL | Under 5 | 737466.128811 | . 30310 World | OWID_WRL | 2004 | 15.144038 | OWID_WRL | 158.124351 | OWID_WRL | Under 5 | 746365.808630 | . 30305 World | OWID_WRL | 2003 | 15.308365 | OWID_WRL | 158.124351 | OWID_WRL | Under 5 | 752025.548675 | . After merging all the datasets together and sorted them: . We can tell that Sub-Saharan Africa has the most deaths of malaria over the years . Now let&#39;s see how many people of each age groups have died in Sub-Saharan Africa from 1990 to 2015. . This is an area plot which I think perfectly shows the distribution of deaths from different age groups in Sub-Saharan Africa from 1990 to 2015. . df_merged_all_ssa=(pd.concat([df_merged_all[df_merged_all.Entity == &quot;Sub-Saharan Africa&quot;]])) df_merged_all_ssa.head(15) fig_ssa = px.area(df_merged_all_ssa, x=&quot;Year&quot;, y=&quot;deaths&quot;,color=&quot;age_group&quot;, labels={ &quot;age_group&quot;: &quot;Different age groups&quot;, &quot;Year&quot;: &quot;Sub-Saharan Africa (1990 to 2015)&quot;, &quot;deaths&quot;: &quot;Total Deaths Count Due To Malaria&quot; }, title=&quot;Malaria deaths count in Sub-Saharan Africa over time measured by area &quot;) fig_ssa .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/2021/09/30/Yiyang-Zhang-823-HW3-Creating-effective-visualizations.html",
            "relUrl": "/2021/09/30/Yiyang-Zhang-823-HW3-Creating-effective-visualizations.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "823 HW2",
            "content": "- . Write a function to generate an arbitrary large expansion of a mathematical expression like &#960;. . (Hint: You can use the standard library decimal or the 3rd party library sympy to do this) . For the first function, I used the sympy library (mpmath). Since the function wasn&#39;t outputting enough decimal places, I set the mp.dps as 1000. . This function has two input conditions to check, either pi(&#960;) or e(euler number). Next, I set a multiplier and multiply them by it. Then I set the precision criterion to get the number of digits after the decimal wanted and turn it into a string. This creates the decimal expansion of a given number. . import math try: from sympy.mpmath import mp except ImportError: from mpmath import mp mp.dps=1000 def create_expansion(precision,number,multiplier): &#39;&#39;&#39; this function takes a number with a multiplier and outputs its decimal expansion with a specific number of decimals input: precision, number of decimals wanted number, number to be expanded(&#39;pi&#39; or &#39;e&#39;) multiplier, multiplier of the number to be expanded returns: an string of decimal expansion of the input number &#39;&#39;&#39; #check if number is pi if number ==&#39;pi&#39;: #create a string of the number(multiplied by the multiplier) expansion with precision as number of decimals str_pi = str(( multiplier*mp.pi)).replace(&#39;.&#39;,&#39;&#39;)[0:precision] return(str_pi) #check if number is e elif number ==&#39;e&#39;: str_e = str(( multiplier*mp.e)).replace(&#39;.&#39;,&#39;&#39;)[0:precision] return(str_e) #type either &#39;pi&#39; or &#39;e&#39; for number in create_expansion function #print(create_expansion(50,&#39;pi&#39;,17)) #print(create_expansion(25,&#39;e&#39;,1)) . File &#34;&lt;tokenize&gt;&#34;, line 20 if number ==&#39;pi&#39;: ^ IndentationError: unindent does not match any outer indentation level . Unit test of create_expansion function: . import unittest class TestNotebook(unittest.TestCase): def test_create_expansion(self): &quot;&quot;&quot;test the expansion of the number we want&quot;&quot;&quot; self.assertEqual(create_expansion(5,&#39;pi&#39;,1),str(31415)) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_create_expansion (__main__.TestNotebook) test the expansion of the number we want ... ok - Ran 1 test in 0.001s OK . &lt;unittest.main.TestProgram at 0x7f8427f6b950&gt; . - . - Write a function to check if a number is prime. Hint: See Sieve of Eratosthenes . For this function, the first criterion I set is to check if the given number is 1 or not. If it is 1, then it is not a prime number. . Next we check if the given number is 2 or not. If it is 2, then it is a prime number. . Then we check if the given number can be divided by 2, if so, it is an even number, thus it is not a prime number. . Lastly, we check from 3 to the positive square root of x so that it only iterate a portion of X values. The step is 2 so no even number other than 2 will participate in this iteration. . This function reduces the run time complexity dramatically from a function without the above steps. . import math def IsPrimeNumber(x): &#39;&#39;&#39; this function takes an input number and test whether it is a prime number or not and outputs an answer x: int, input to be tested returns: an answer (True or False) &#39;&#39;&#39; # exclude 1 which is not prime if x == 1: return False # take out 2 as a base case elif x == 2: return True elif x % 2 == 0: return False else: # iterate through 3 to the positive square root of x to see if x can be divided by any, step is 2 which excludes all even number. for y in range(3, int(math.sqrt(x) + 1), 2): # if x can be divided, then x is not prime if x % y == 0: return False # if x can not be divided, then x is a prime number return True . Unit test of IsPrimeNumber function: . class TestNotebook(unittest.TestCase): def test_IsPrimeNumber(self): &quot;&quot;&quot;test IsPrimeNumber&quot;&quot;&quot; self.assertFalse(IsPrimeNumber(1)) self.assertTrue(IsPrimeNumber(2)) self.assertFalse(IsPrimeNumber(51)) self.assertTrue(IsPrimeNumber(1373)) self.assertFalse(IsPrimeNumber(33333)) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_IsPrimeNumber (__main__.TestNotebook) test IsPrimeNumber ... ok - Ran 1 test in 0.001s OK . &lt;unittest.main.TestProgram at 0x7f84390c1d10&gt; . - . - Write a function to generate sliding windows of a specified width from a long iterable (e.g. a string representation of a number) . Then we have the window function which generates sliding windows of a specified width from a long iterable. This one is pretty straight forward, it returns a list of sliding windows(substrings of the input string). One interesting part I did in this function is that I added a list called &#39;seen&#39; which records every slinding windows(substrings) we have seen so we will not have repeated slinding windows(substrings) in the output list(non_repeated). This will reduce the run time complexity since a lof of redundant values will be checked later if the specified width is too small. . def window(seq, width): &#39;&#39;&#39; this function takes an input string and returns all the substrings with the length wanted seq: str, input string to be sliced into &#39;windows&#39; width: length of windows wanted returns: all the substrings(windows) with the length wanted(width) &#39;&#39;&#39; #exclude the number before the decimal seq=seq[1:] #create two lists, seen and non_repeated seen, non_repeated =[], [] #iterate through the input string for i in range(0,len(seq)-(width-1)): #create windows of given width t = seq[i:i+width] #excluded repeated windows if t not in seen: #collect non-repeated windows non_repeated.append(t) #collect repeated windows as seen windows seen.append(t) #return a list of non-repeated windows return list(non_repeated) . print(window(str(12345678), 4)) . [&#39;2345&#39;, &#39;3456&#39;, &#39;4567&#39;, &#39;5678&#39;] . Unit test of window function: . class TestNotebook(unittest.TestCase): def test_window(self): &quot;&quot;&quot;test window.&quot;&quot;&quot; self.assertEqual(window(str(12345678), 4), [&#39;2345&#39;, &#39;3456&#39;, &#39;4567&#39;, &#39;5678&#39;]) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_window (__main__.TestNotebook) test window. ... ok - Ran 1 test in 0.001s OK . &lt;unittest.main.TestProgram at 0x7f8427f0f710&gt; . - . Now use these helper functions to write the function that you need. . Write a unit test for this final function, given that the first 10-digit prime in the expansion e is 7427466391. . Finally, solve the given problem. . This function uses all of the helper functions I wrote above. . It iterates through the list of numbers generated by the window function from the string of the expansion of the given number generated by the create_expansion function, and then checks whether every number of this list is a prime number using the IsPrimeNumber function. . Lastly, it returns the first prime number of the expansion with the wanted length of digits. . def give_prime_expansion(digits_of_number, input_number, multiplier, length_of_prime): &#39;&#39;&#39; this function takes the digits of number, an input number(&#39;pi&#39; or &#39;e&#39;), a multiplier and the length of prime number we want, returns the first prime number in the expansion. digits_of_number: the number of decimal digits the input_number: pi or e multiplier: a multiplier of the input number length_of_prime: the length of the prime number we want returns: the first prime number in the decimal expansion of the input number &#39;&#39;&#39; #iterte through the list of windows of numbers for numbers in window(create_expansion(digits_of_number,input_number,multiplier),length_of_prime): #check if the number is prime if IsPrimeNumber(int(numbers)): #output the number we want print(f&quot;The first {length_of_prime}-digit prime number in the decimal expansion of {multiplier} {input_number} is: &quot;) return(int(numbers)) break . Thus we can find the first 10-digit prime in the decimal expansion of 17&#960;. . print(give_prime_expansion(99,&#39;pi&#39;,17,10)) . The first 10-digit prime number in the decimal expansion of 17 pi is: 8649375157 . Unit test of give_prime_expansion function: . class TestNotebook(unittest.TestCase): def test_give_prime_expansion(self): &quot;&quot;&quot;test test_give_prime_expansion .&quot;&quot;&quot; self.assertEqual(give_prime_expansion(120,&#39;e&#39;,1,10), 7427466391) self.assertEqual(give_prime_expansion(99,&#39;pi&#39;,1,5), 14159) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_give_prime_expansion (__main__.TestNotebook) test test_give_prime_expansion . ... . The first 10-digit prime number in the decimal expansion of 1 e is: The first 5-digit prime number in the decimal expansion of 1 pi is: . ok - Ran 1 test in 0.008s OK . &lt;unittest.main.TestProgram at 0x7f8427f19d90&gt; .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/2021/09/17/Yiyang-Zhang-823-HW2.html",
            "relUrl": "/2021/09/17/Yiyang-Zhang-823-HW2.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "823 HW1",
            "content": "Euler 10 . The sum of the primes below 10 is 2 + 3 + 5 + 7 = 17. . Find the sum of all the primes below two million. . Solution: . I decided to write a function to test whether the given number is a prime number or not and then sum up all of the numbers that are prime in the given range. . One intersting thing I did to my code is that, after the initial try, I find that my code is taking too long so to output the answer so I decided to reduce the run time. . I set the range in my IsPrimeumber function from (3, x) to (3, int(math.sqrt(x)+1),2) so that it only iterate to the square root of X and the step is 2 so no even number other than 2 will participate in the iteration. . This reduces the run time dramatically. . import math def IsPrimeNumber(x): &#39;&#39;&#39; this function takes an input number and test whether it is a prime number or not and outputs an answer x: int, input to be tested returns: an answer (True or False) &#39;&#39;&#39; # exclude 1 which is not prime if x == 1: return False # take out 2 as a base case elif x==2: return True elif x % 2==0: return False else: # iterate through 2 to x to see if x can be divided by any for y in range(3, int(math.sqrt(x)+1),2): # if x can be divided, then x is not prime if x % y == 0: return False # if x can not be divided, then x is a prime number return True # set the original sum value as 0 sum = 0 # set the upper range as max max = 2000000 # iterate through 2 to max for z in range(2, max): # if z is a prime number, add z to sum if IsPrimeNumber(z): sum += z # output the sum print(f&quot;The sum of prime numbers in this range is {sum}&quot;) . Euler 31 . In the United Kingdom the currency is made up of pound (&#163;) and pence (p). There are eight coins in general circulation: . 1p, 2p, 5p, 10p, 20p, 50p, &#163;1 (100p), and &#163;2 (200p). . It is possible to make &#163;2 in the following way: . 1&#215;&#163;1 + 1&#215;50p + 2&#215;20p + 1&#215;5p + 1&#215;2p + 3&#215;1p . How many different ways can &#163;2 be made using any number of coins? . Solution: . This solution is fairly simple, I calculated every possible combination of different coin with a huge loop, from the outside layer of 100p counts all the way to the inner layer of 2p counts. . After this huge loop, I added the last count value of 200p which is only 1, to the final count. . One intersting fact about this one is that I actually didn&#39;t find anything intersting to do to it. So I will just leave it like this. . # set a count value count = 0 # set the last count value(200p which is only 1) to be added in the end last_count=1 &#39;&#39;&#39; This loop calculates the amount of each type of coins to be divided in corresponding money range(200,100,50 etc) &#39;&#39;&#39; # range: number of 100p coins, maximum 2, range(3) for a in range(3): # range: number of 50p counts for b in range(int(1+(200-100*a)/50)): # range: number of 20p counts for c in range(int(1+(200-100*a-50*b)/20)): # range: number of 10p counts for d in range(int(1+(200-100*a-50*b-20*c)/10)): # range: number of 5p counts for e in range(int(1+(200-100*a-50*b-20*c-10*d)/5)): # range: number of 2p counts for f in range(int(1+(200-100*a-50*b-20*c-10*d-5*e)/2)): count += 1 # Now we have calculated all kinds of combination of coins to combine 200, # equals count # Added the last 1 for the 200p case final_count=count+last_count #Output the result print (f&quot;The final count is {final_count}&quot;) . The final count is 73682 . euler 146 . The smallest positive integer n for which the numbers n2+1, n2+3, n2+7, n2+9, n2+13, and n2+27 are consecutive primes is 10. The sum of all such integers n below one-million is 1242490. . What is the sum of all such integers n below 150 million? . Solution . For this question, I did a couple of functions to be used in my loop to calculate. . Basically, I iterate through 10 to cap(the maximum value to be tested) with a step of 2 (excludes all even numbers) to test whether the numbers n2+1, n2+3, n2+7, n2+9, n2+13, and n2+27 are consecutive primes using the isAsked function which test each one using the isPrime function. . One interesting thing I did is that I created a list which basically records all the prime numbers, so like for the N^2 + 27 maximum case we can only test whether it can be divided by these prime numbers. This will excluded all other unnecessary numbers to be tested and thus save us a lot of run time. . import math # Set the maximum integar value cap = int(150e6) # Create and empty list to store the prime numbers we found primes = [] &#39;&#39;&#39; A function that list all the prime numbers we found n: int, input number primes: list, output n into &#39;&#39;&#39; def listPrimes(n, primes): # base case if n == 1: return [] # other cases for p in primes: if p &gt; int(math.sqrt(n)): break if n % p == 0: return primes primes.append(n) return primes # For N^2 + 27 maximum case for i in range(2, int(math.sqrt(cap**2+27)+1)): primes = listPrimes(i, primes) print(primes) &#39;&#39;&#39; A function that tests whether n is a prime number or not, returns True or False n: int, number to be tested &#39;&#39;&#39; def isPrime(n): for p in primes: if n % p == 0: return False if p &gt; math.sqrt(n): return True return True &#39;&#39;&#39; A function that test if given number n is asked to be calculated in the isPrime function above Returns True or False for each n n: int, input number to be tested &#39;&#39;&#39; def isAsked(n): if not isPrime(n**2+1): return False if not isPrime(n**2+3): return False if isPrime(n**2+5): return False if not isPrime(n**2+7): return False if not isPrime(n**2+9): return False if isPrime(n**2+11): return False if not isPrime(n**2+13): return False # only interate by setp of 2 to avoid even numbers in this range for x in range(n**2+15, n**2+27, 2): if isPrime(x): return False if not isPrime(n**2+27): return False return True # declare a sm value sm = 0 #loop through 10 to cap(max) with a step of 2 for n in range(10, cap, 2): if isAsked(n): print(n) sm += n print(f&quot;sum = {sm}&quot;) .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/2021/09/03/Yiyang-Zhang-823-HW1.html",
            "relUrl": "/2021/09/03/Yiyang-Zhang-823-HW1.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yiyangzhang2020.github.io/yz628-823-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://yiyangzhang2020.github.io/yz628-823-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yiyangzhang2020.github.io/yz628-823-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}